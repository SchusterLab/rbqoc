\section{Background \label{sec:background}}
In this section, we
review the QOC problem statement
and describe the ALTRO solver \cite{howell2019altro}.
QOC concerns a vector $\mathbf{a}(t)$ of time-dependent control fields
that steer the evolution of a quantum state $\ket{\psi(t)}$.
The evolution of the state is governed by the time-dependent
Schr{\"o}dinger equation (TDSE),
\begin{equation}
  i \hbar \frac{d}{dt} \ket{\psi(t)} = H(\mathbf{a}(t), t) \ket{\psi(t)}.
  \label{eq:tdse}
\end{equation}
The Hamiltonian $H(\mathbf{a}(t), t)$ is determined by the quantum system and the external control fields.
The QOC problem is to find the
controls that minimize a functional $J[\mathbf{a}(t)]$,
which we call the objective.
To make the problem numerically tractable,
the quantum state and controls are discretized into $N$ time steps,
$\ket{\psi(t_{k})} \to \ket{\psi_k}$ and $\mathbf{a}(t_{k}) \to \mathbf{a}_k$ where
$t_{k + 1} = t_{k} + \Delta t$ and $k \in \{1, ..., N\}$.
In the case of a single state-transfer problem, the objective is
the infidelity of the time-evolved final state $\ket{\psi_N}$ and
the intended target state $\ket{\psi_T}$
\textcolor{blue}{[to me, ``f" would mean ``final" rather than target...]}
\textcolor{red}{[changed $f$ to $T$]},
$J(\mathbf{a}) = 1 - {\lvert \braket{\psi_{T}}{\psi_{N}(\mathbf{a})} \rvert}^{2}$.
Standard QOC solvers compute derivatives of the objective $\nabla J(\mathbf{a})$,
which can easily be used to implement first-order optimization methods
\cite{machnes2015tunable, khaneja2005optimal, leung2017speedup, goerz2019krotov}.

Alternatively, the QOC problem can be formulated as a trajectory optimization problem 
and solved using specialized solvers developed by the robotics community
\cite{Schulman13, Tedrake16, Hereid2017FROST, howell2019altro}.
The objective $J(\mathbf{a})=\sum_k \ell_{k}(\mathbf{x}_{k}, \mathbf{u}_{k})$
is expressed in terms of the
cost function at each time step $\ell_k$, where
$\mathbf{x}_{k}$ is the augmented state vector
and $\mathbf{u}_{k}$ is the augmented control vector.
We use the term \emph{augmented} because these
vectors contain all of the relevant variables in the optimization problem,
not just the quantum state and the control fields,
for an example see Section \ref{sec:fluxonium}.
The augmented control contains all variables that the experimentalist
may manipulate, and the augmented state contains all variables that depend
on those in the augmented control.
The variables in the augmented states depend on those in the augmented
controls as defined by the differential equations governing the physical system, which are
encoded in the discrete relation
$\mathbf{x}_{k + 1} = \mathbf{f}(\mathbf{x}_{k}, \mathbf{u}_{k})$.
For QOC, the discrete dynamics function $\mathbf{f}(\mathbf{x}_{k}, \mathbf{u}_{k})$
propagates the quantum state by
integrating the TDSE \eqref{eq:tdse} with the given control fields
using Runge-Kutta methods \cite{jorgensen2011numerical}
or exponential integrators \cite{auer2018magnus, berland2006solving, einkemmer2017performance,
  shillito2020fast}.
\textcolor{blue}{[”discrete dynamics function” is not physics language.
    Relate f to Schrodinger equation first, then give it the un- familiar name.]}
\textcolor{red}{[I think the sentence flows better with "The discrete dynamics function
    $f(., .)$ propagates" rather than "$f(., .)$ propagates... By the way, $f(., .)$ is
    called the ddf". Furthermore, I cannot produce a sentence which relates
    the TDSE dependence of $\ket{\psi_{k}}$ and $\mathbf{a}_{k}$ to each other
    and then introduces $\mathbf{x}_{k + 1} = \mathbf{f}(\mathbf{u}_{k}, \mathbf{x}_{k})$
    because then I feel it needs to be explained again that $\ket{\psi_{k}}$
    and $\mathbf{a}_{k}$ are contained in $\mathbf{x}_{k}$ and $\mathbf{u}_{k}$.
    I think introducing the specific case of QOC later in "For QOC"
    is less verbose and more words will detract from understanding in this instance.]}
\textcolor{blue}{[Separate point: if x contains components other than the quantum state,
    then what does "obeying the physics of the system” mean?]}
\textcolor{red}{[everything time evolves under a differential equation even if it's
    really simple $\mathrm{d}a/\mathrm{d}t = \dot{a}$]}
\textcolor{blue}{[definition of $\mathbf{f}$ should be separated from its numerical
    implementation?]}
\textcolor{red}{[I think that the lead in "For QOC," makes it evident that this is
    a specific case,
    similar to how above it says "For example," when talking about specific components
    of the state and control vectors]}.

Additional constraints on the augmented controls and states
are formulated as inequalities  $\mathbf{g}_{k}(\mathbf{x}_{k}, \mathbf{u}_{k}) \leq \mathbf{0}$
or equalities $\mathbf{h}_{k}(\mathbf{x}_{k}, \mathbf{u}_{k}) = \mathbf{0}$.
\textcolor{blue}{[How to understand the $k$? labels here?]}
\textcolor{red}{[they denote the time step, which i think is consistent with everything else]}
The constraint functions $\mathbf{g}_k$ and $\mathbf{h}_k$ may be vector-valued to encode multiple constraints,
and equalities and inequalities are understood component-wise. Each constraint's \emph{violation}
is defined as the magnitude of its deviation:
$\textrm{max}(g(\cdot), 0)$ or $\abs{h(\cdot)}$, where $g$ and $h$
are individual components of constraint functions $\mathbf{g}_{k}$ and $\mathbf{h}_{k}$,
respectively.
\textcolor{blue}{[Is this the right place for this definition in the text.
    Seems unmotivated/out of place right now?]}
\textcolor{red}{[Yes, first place it comes up in ALM paragraph is a worse place to introduce it]}
Stated concisely, the trajectory optimization problem is
\begin{mini!}[2]
  {\substack{\mathbf{x}_1, \ldots, \mathbf{x}_N \\\mathbf{u}_1, \ldots, \mathbf{u}_{N - 1}}}{
    \ell_{N}(\mathbf{x}_{N}) + \sum^{N - 1}_{k = 1}\ell_{k}
    (\mathbf{x}_k,\mathbf{u}_k)}{}{} \label{eq:gcostfun}
  \addConstraint{\mathbf{x}_{k+1} = \mathbf{f}(\mathbf{x}_k,\mathbf{u}_k) \quad \forall \ k}
  \label{eq:gdyncon}
  \addConstraint{\mathbf{g}_k(\mathbf{x}_k,\mathbf{u}_k) \leq \mathbf{0} \quad \forall \ k}
  \label{eq:ineqcon}
  \addConstraint{\mathbf{h}_k(\mathbf{x}_k,\mathbf{u}_k) = \mathbf{0} \quad \forall \ k.}
  \label{eq:eqcon}
\end{mini!}
\textcolor{blue}{[Can't we just define $\mathbf{u}_N=\mathbf{0}$
    to simplify the expression in (2a)?]}
\textcolor{red}{[I'm not certain that is the same thing as
    saying that $\mathbf{u}_{N}$ is undefined.
    What I think you are saying is $\ell_{N}(\mathbf{x}_{N},
    \mathbf{u}_{N} = \mathbf{0})$, but what is really going
    on is that there is no dependence at all $\ell_{N}(\mathbf{x}_{N})$]}

Standard techniques for solving \eqref{eq:gcostfun}-\eqref{eq:eqcon} typically
fall into two categories: direct methods \cite{Hargraves87, kelly2017introduction}
and indirect methods \cite{betts1998survey}. For indirect methods,
the augmented controls are the \emph{decision variables}, i.e., the
variables the optimizer adjusts to solve the problem.
The augmented states are obtained from the augmented controls using the discrete dynamics function,
and they are used to evaluate derivatives of the cost functions.
Then, the derivative information is employed to update the augmented controls.
This approach is taken by standard QOC solvers such as GOAT \cite{machnes2015tunable},
GRAPE \cite{khaneja2005optimal, leung2017speedup}, and Krotov's method \cite{goerz2019krotov}.
Conversely, direct methods treat both the augmented controls and states as decision
variables. In addition to minimizing the cost functions, the optimizer uses derivative information
for the discrete dynamics function to satisfy the dynamics constraint
\eqref{eq:gdyncon} to a specified tolerance.
In this sense, the TDSE \eqref{eq:tdse} is a constraint that may be violated
for intermediate steps of the optimization, where the states need not be physical.
The direct approach lends itself to a nonlinear program formulation, for which
a variety of general-purpose solvers exist \cite{gill2005snopt, wachter2006implementation}.
Recent state-of-the-art solvers, such as ALTRO,
combine the indirect and direct methods in a two-stage approach.
First, ALTRO employs an indirect solving stage using
the iterative linear-quadratic regulator (iLQR) algorithm
\cite{Li2004a} as the internal solver of an augmented Lagrangian method (ALM)
\cite{lantoine2012hybrid, plancher2017constrained, nocedal2006numerical}.
In the second direct stage, ALTRO uses a projected Newton method
\cite{bertsekas1982projected, rao1998application}.
Next, we provide a more detailed summary of these two stages.

iLQR is an indirect method
for minimizing the objective subject to the dynamics constraint,
i.e., solving \eqref{eq:gcostfun}-\eqref{eq:gdyncon}.
\textcolor{blue}{[Start with the familiar or the understandable explanation.
    The imposing sounding names can be given afterwards. (Purely psychology.)]}
\textcolor{red}{[resolved]}
First, iLQR uses an initial guess for the augmented controls to obtain the 
augmented states with the discrete dynamics function.
iLQR then constructs quadratic models for each cost function using
their zeroth-, first- and second-order derivatives in a Taylor expansion
about the current augmented controls and states.
These models are used in a recurrence relation between time steps
to derive the locally optimal update for the augmented controls.
\textcolor{blue}{[for some quantity (which?) at consecutive time steps]}
\textcolor{red}{[resolved]}
Finally, a line search \cite{zhang2006global}
is performed in the direction of this update to ensure a
decrease in the objective.
\textcolor{blue}{[again, I suspect that (2a) is not the ``objective"; rather the objective
    is what I know as cost function, which would exclude the minimization that is part of (2a).]}
\textcolor{red}{[resolved, i introduce objective earlier now]}
This procedure
is repeated until convergence is reached.

While indirect solvers like iLQR are computationally
efficient and maintain high accuracy for the discrete dynamics
throughout the optimization, they cannot handle
nonlinear equality and inequality
constraints \eqref{eq:ineqcon}-\eqref{eq:eqcon}.
\textcolor{blue}{[Seems like my edit attempts and understanding
    of the two-stage process are still not appropriate.
    On the previous page, it seemed like the projected Newton
    method was the final stage and a direct method at that.
    However, here it sounds now like it is not good enough
    and we need yet another method? I'm confused.]}
\textcolor{red}{[removed discussion of projected gradient methods]}
For QOC, a popular approach to handle such constraints
is to add the constraint functions to the objective
\cite{heeres2017implementing, leung2017speedup, reinhold2019controlling}.
\textcolor{blue}{[(2a) is of the form $\text{min}_x L(x)$
    do you really mean to add to the minimum? Or only to the cost function $L(x)$?]}
\textcolor{red}{[resolved, i introduce objective earlier now]}
However, this strategy does not guarantee that the constraints
are satisfied as the solver trades
minimization of the cost functions and constraint functions against each other.
ALM remedies this issue by adaptively adjusting a Lagrange multiplier estimate
for each constraint function to ensure the constraints are satisfied.
ALM adds terms that are linear and quadratic in the constraint functions
to the objective. Then, the new objective is minimized with
iLQR. If the solution obtained with iLQR does not satisfy the constraints,
the prefactors for the constraint terms in the objective are increased
intelligently and the procedure is repeated.

ALM converges superlinearly, but poor numerical conditioning may lead
to small decreases in the constraint violations near the locally optimal solution
\cite{bertsekas1996constrained}.
To address this shortcoming, ALTRO
projects the solution from the ALM stage onto the constraint manifold using
a (direct) projected Newton method, achieving ultra-low
constraint violations $\sim 10^{-8}$.
For more information on the details of the ALTRO
solver, see \cite{howell2019altro, Jackson2020altroc}.

As opposed to standard QOC solvers, ALTRO
can satisfy constraints
on both the augmented controls and states to tight tolerances.
This advantage is crucial for this work, where multiple medium-priority cost functions
are minimized subject to many high-priority constraints.

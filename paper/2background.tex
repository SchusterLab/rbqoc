\section{QOC + AL-iLQR \label{sec:background}}
In this section we introduce the notation
we will use throughout the paper,
review the quantum optimal control problem statement,
and introduce the trajectory optimization framework.
Quantum optimal control concerns the evolution of
a quantum state $\ket{\psi(t)}$ governed by the time-dependent
Schr{\"o}dinger equation (TDSE),
\begin{equation}
  i \hbar \frac{d}{dt} \ket{\psi} = H(u(t), t) \ket{\psi}
  \label{eq:tdse}
\end{equation}
The Hamiltonian
has an arbitrary dependence on the possibly multi-valued controls $u(t)$.
The controls are so called because they are the means the experimentalist has to
act on the system. Numerical quantum optimal control techniques make
the problem tractable by discretizing the problem into $N$
knot points (time steps). Typical explicit integration techniques for the TDSE include
exponential integrators \cite{auer2018magnus, berland2005solving, einkemmer2017performance}
and Runge-Kutta integrators \cite{jorgensen2011numerical}.

Quantum optimal control seeks the control
parameters that minimize a functional $J(u(t))$.
In the simplest case the functional is
the infidelity between the inital state evolved
to the final knot point and the target state
$J = 1 - {\lvert \braket{\psi_{f}}{\psi_{N}(u(t))} \rvert}^{2}$.
In general $J$ is a linear combinaion of cost functions on the state
as well as cost functions on the controls \cite{leung2017speedup}.
Typical quantum optimal control
algorithms employ automatic differentiation
to compute gradients of the functional $\nabla_{u} J(u)$,
which can easily be used to implement first-order optimization methods
\cite{clarkson2010coresets, hauswirth2016projected}.

Alternatively, the QOC problem can be formulated as a trajectory optimization problem 
and solved using a variety of specialized solvers developed by the robotics community
\cite{Schulman13,Tedrake16,Hereid2017FROST,howell2019altro}.
Trajectory optimization problems are typically of the following form: 
\begin{mini}[2]
    {x_{0:N},u_{0:N-1}}{\ell_f(x_N) + \sum_{k=0}^{N-1} \ell(x_k,u_k) }{}{}
    \addConstraint{x_{k+1} = f(x_k,u_k)}
    \addConstraint{g_k(x_k,u_k)}{\leq 0}
    \addConstraint{h_k(x_k,u_k)}{=0}
    \label{opt:discrete_trajopt},
\end{mini}
where $\ell_f$ and $\ell$ are the final and stage cost functions, $x_k \in \R^n$ and
$u_k \in \R^m$ are the state and input control variables, $f(x_k,u_k)$ is the discrete
dynamics function, and $g_k(x_k,u_k)$ and $h_k(x_k,u_k)$ are the inequality
and equality constraints, potentially including initial and final conditions,
at time step $k$.

Many techniques have been proposed for solving \eqref{opt:discrete_trajopt}. Standard 
methods include direction collocation \cite{Hargraves87} and differential-dynamic programming
(DDP) \cite{Mayne1966a}. Recent state-of-the-art solvers, such as ALTRO \cite{howell2019altro},
have combined principles from both of these approaches.

ALTRO uses iterative LQR (iLQR) \cite{Li2004a} as the internal solver of an augmented 
Lagrangian method (ALM). iLQR solves an unconstrained trajectory optimization problem 
using a backward Riccati recursion to derive a closed-loop linear feedback law about the current 
trajectory. By simulating the system forward with the feedback law, the trajectory is 
brought closer to the (locally) optimal trajectory. DDP-based solvers such as iLQR
are popular since they are very computationally efficient, are always dynamically feasible,
and provide a locally stabilizing closed-loop control policy about the optimal trajectory. However,
standard implementations have no ability to deal with nonlinear equality and inequality 
constraints. ALM handles constraints by successively solving unconstrained minimization 
problems of the form:
\begin{mini}[2]
    {z}{f(z) + \lambda^T c(z) + \half c(z)^T I_\mu c(z)}{}{}
    \label{opt:alm}
\end{mini}
where $f(z)$ is the objective function, $c(z) : \R^n \mapsto \R^p$ is the constraint 
function, $\lambda \in \R^p$ is a Lagrange multiplier estimate, and $I_\mu$ is a diagonal matrix
of penalty weights, $\mu$, whose magnitudes depend on whether the corresponding constraint is active or inactive.
For ALTRO, $f(z)$ is the objective function of \eqref{opt:discrete_trajopt}, $c(z)$ is the 
concatenation of $g_k$ and $h_k$, and $z$ is the concatenation of the states and controls 
across all time steps. After minimizing \eqref{opt:alm} using iLQR,
the Lagrange multiplier estimates are updated according to,
\begin{equation}
	\lambda \gets \lambda + \mu c(z) ,
\end{equation}
the penalty terms are updated, and the process repeats until convergence.

ALM converges superlinearly but tends to exhibit slow constraint convergence near the optimal 
solution due to poor numerical conditioning. To address this shortcoming, ALTRO provides a 
solution-polishing phase that takes 1-2 Newton steps on the active constraint set to provide 
machine-precision constraint satisfaction. For more information on the details of the ALTRO
solver, see \cite{howell2019altro, Jackson2020altroc}.

The ALTRO method has fast tail-convergence and
allows us to impose arbitrary constraints without restricting
the optimization to the constraint manifold.
This contrasts typical first-order QOC methods that employ the same
update procedure for the duration of optimization
and employ projected gradient methods
which restrict the optimization to the constraint manifold.
To see how to use the ALTRO method for QOC, we will present
a concrete example in the following section.


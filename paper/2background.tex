\section{Background \label{sec:background}}
In this section, we
review the QOC problem statement
and describe the ALTRO solver \cite{howell2019altro}.
QOC concerns a vector of time-dependent control parameters $u(t)$
that steer the evolution of a (quantum) state $\ket{\psi(t)}$.
The evolution of the state is governed by the time-dependent
Schr{\"o}dinger equation (TDSE),
\begin{equation}
  i \hbar \frac{d}{dt} \ket{\psi(t)} = H(u(t), t) \ket{\psi(t)}.
  \label{eq:tdse}
\end{equation}
The Hamiltonian $H(u(t), t)$ is determined by the quantum system.
The QOC problem is to find the
controls that minimize a functional $J(u(t))$.
To make the problem numerically tractable,
the controls and state are discretized into $N$ knot points (time steps).
In the case of a single state-transfer problem, the functional is
the infidelity of the desired final state and the initial state evolved
to the final knot point,
$J(u) = 1 - {\lvert \braket{\psi_{f}}{\psi_{N}(u)} \rvert}^{2}$.
In general, $J(u)$ is a linear combination of cost functions on the state
as well as cost functions on the controls.
Standard QOC solvers compute derivatives of the functional $\nabla_{u} J(u)$,
which can easily be used to implement first-order optimization methods
\cite{machnes2015tunable, khaneja2005optimal, leung2017speedup, goerz2019krotov}.

Alternatively, the QOC problem can be formulated as a trajectory optimization problem 
and solved using a variety of specialized solvers developed by the robotics community
\cite{Schulman13, Tedrake16, Hereid2017FROST, howell2019altro}.
The functional $J(u)$ is divided into its constituent
cost functions at each knot point $\ell_{k}(x_{k}, u_{k})$,
where $k \in \{1, ..., N\}$ denotes the knot point,
$x_{k}$ is the augmented state vector,
and $u_{k}$ is the augmented control vector.
The augmented state contains all relevant variables
that are dependent upon the controls, for example the state
$\ket{\psi_{k}} \subseteq x_{k}$.
The augmented state trajectory obeys the physics of the system if
the dynamics constraint is satisfied
$x_{k + 1} = f(x_{k}, u_{k})$. For QOC, the discrete
dynamics function $f(x_{k}, u_{k})$ propagates the state by
integrating the TDSE \eqref{eq:tdse} with Runge-Kutta methods \cite{jorgensen2011numerical}
or exponential integrators \cite{auer2018magnus, berland2006solving, einkemmer2017performance,
shillito2020fast}.

Additional constraints on the augmented controls and augmented states
are encoded in constraint functions.
The constraint functions are put into a form such that, when the constraint is satisfied,
inequality constraint functions obey $g_{k}(x_{k}, u_{k}) \leq 0$ and
equality constraint functions obey $h_{k}(x_{k}, u_{k}) = 0$. The constraint
functions may be vector-valued to encode multiple constraints of each type.
The constraint function's violation is the magnitude by which it is not satisfied:
$\textrm{max}(g(\cdot), 0)$ or $\textrm{max}(\abs{h(\cdot)})$
where $\textrm{max}(\cdot)$ and $\abs{\cdot}$ act element-wise.
Stated concisely, the trajectory optimization problem is,
\begin{mini!}[2]
  {x_{1:N},u_{1:N-1}}{\ell_{N}(x_N) + \sum_{k=1}^{N-1}
    \ell_{k}(x_k,u_k)}{}{} \label{eq:gcostfun}
    \addConstraint{x_{k+1} = f(x_k,u_k) \ \forall \ k} \label{eq:gdyncon}
    \addConstraint{g_k(x_k,u_k) \leq 0 \ \forall \ k} \label{eq:ineqcon}
    \addConstraint{h_k(x_k,u_k) = 0 \ \forall \ k} \label{eq:eqcon}
\end{mini!}

Standard techniques for solving \eqref{eq:gcostfun}-\eqref{eq:eqcon} typically
fall into two categories: direct methods \cite{Hargraves87, kelly2017introduction}
and indirect methods \cite{betts1998survey}. For indirect methods,
the augmented controls are the decision variables--the
variables the optimizer adjusts to solve the problem.
The augmented states are obtained from the augmented controls using the discrete dynamics function,
and are then used to evaluate derivatives of the cost functions \eqref{eq:gcostfun}.
Then, the derivative information is employed to update the augmented controls.
This approach is taken by standard QOC solvers such as GOAT \cite{machnes2015tunable},
GRAPE \cite{khaneja2005optimal, leung2017speedup}, and Krotov's method \cite{goerz2019krotov}.
Conversely, direct methods treat both the augmented controls and the augmented states as decision
variables. In addition to minimizing the cost functions, the optimizer uses derivative information
for the discrete dynamics function
to satisfy the dynamics constraint \eqref{eq:gdyncon} to a specified tolerance.
In this sense, the TDSE \eqref{eq:tdse} is a constraint that may be violated
for intermediate steps of the optimization, where the states need not be physical.
The direct approach lends itself to a nonlinear program formulation, for which
a variety of general-purpose solvers exist \cite{gill2005snopt, wachter2006implementation}.
Recent state-of-the-art solvers, such as ALTRO,
have combined principles from both of these approaches. ALTRO uses an
iterative linear-quadratic regulator (iLQR) algorithm
\cite{Li2004a} as the internal solver of an augmented Lagrangian method (ALM)
\cite{lantoine2012hybrid, plancher2017constrained}
and employs a projected Newton method \cite{bertsekas1982projected, rao1998application}
in its final solving stage.

iLQR is an indirect method
for solving the dynamically constrained
trajectory optimization problem \eqref{eq:gcostfun}-\eqref{eq:gdyncon},
and its update procedure is based on the differential-dynamic-programming approach
\cite{mayne1966a}.
First, iLQR uses an initial guess for the augmented controls to obtain the augmented
states with the discrete dynamics function.
iLQR then constructs quadratic models for each cost function using
their zeroth-, first- and second-order derivatives in a Taylor expansion
about the current augmented controls and augmented states.
These models are used to derive a recurrence relation between knot points
which gives the locally optimal update for the augmented controls.
Finally, a line search \cite{zhang2006global}
is performed in the direction of this update to ensure a
decrease in the objective \eqref{eq:gcostfun}. This procedure
is repeated until convergence.

Indirect solvers such as iLQR are popular
because they are very computationally efficient and maintain
high accuracy for the discrete dynamics throughout the optimization.
However, standard implementations cannot handle
nonlinear equality and inequality
constraints \eqref{eq:ineqcon}-\eqref{eq:eqcon}.
Projected gradient methods are a typical approach
to handle constraints
\cite{clarkson2010coresets, hauswirth2016projected, morzhin2019minimal,
  nikolskii2007convergence}. Unfortunately, within the indirect framework,
they can only be used for constraints on the augmented controls,
not the augmented states.
Another technique, which is popular for QOC \cite{heeres2017implementing, leung2017speedup,
reinhold2019controlling},
is to add the constraint functions to the objective
\eqref{eq:gcostfun}. 
This strategy does not guarantee that the constraints
are satisfied as the solver trades
minimization of the cost functions and constraint functions against each other.
ALM remedies this issue by adaptively adjusting a Lagrange multiplier estimate
for each constraint function to ensure the constraints are satisfied.
ALM adds terms that are linear and quadratic in the constraint functions
to the objective. Then, the new objective is minimized with
iLQR. If the solution obtained with iLQR does not satisfy the constraints,
the prefactors for the constraint terms in the objective are increased
intelligently and the procedure is repeated.

ALM converges superlinearly, but poor numerical conditioning may lead
to small decreases in the constraint violations near the locally optimal solution
\cite{bertsekas1996constrained}.
To address this shortcoming, ALTRO
projects the solution from the ALM stage onto the constraint manifold using
a (direct) projected Newton method, achieving ultra-low
constraint violations $\sim 10^{-8}$.
For more information on the details of the ALTRO
solver, see \cite{howell2019altro, Jackson2020altroc}.

As opposed to standard QOC solvers, ALTRO
can satisfy constraints
on both the augmented controls and the augmented states to tight tolerances.
This advantage is crucial for this work, where multiple medium-priority cost functions
are minimized subject to many high-priority constraints.
